{"cells":[{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\nsimpleData = [(\"James\",34,\"2006-01-01\",\"true\",\"M\",3000.60),\n    (\"Michael\",33,\"1980-01-10\",\"true\",\"F\",3300.80),\n    (\"Robert\",37,\"06-01-1992\",\"false\",\"M\",5000.50)\n  ]\n\ncolumns = [\"firstname\",\"age\",\"jobStartDate\",\"isGraduated\",\"gender\",\"salary\"]\ndf = spark.createDataFrame(data = simpleData, schema = columns)\ndf.printSchema()\ndf.show(truncate=False)\n\nfrom pyspark.sql.functions import col\nfrom pyspark.sql.types import StringType,BooleanType,DateType\ndf2 = df.withColumn(\"age\",col(\"age\").cast(StringType())) \\\n    .withColumn(\"isGraduated\",col(\"isGraduated\").cast(BooleanType())) \\\n    .withColumn(\"jobStartDate\",col(\"jobStartDate\").cast(DateType()))\ndf2.printSchema()\n\ndf3 = df2.selectExpr(\"cast(age as int) age\",\n    \"cast(isGraduated as string) isGraduated\",\n    \"cast(jobStartDate as string) jobStartDate\")\ndf3.printSchema()\ndf3.show(truncate=False)\n\ndf3.createOrReplaceTempView(\"CastExample\")\ndf4 = spark.sql(\"SELECT STRING(age),BOOLEAN(isGraduated),DATE(jobStartDate) from CastExample\")\ndf4.printSchema()\ndf4.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"a5587253-cd53-4859-9e0b-9f645a08e359","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- firstname: string (nullable = true)\n |-- age: long (nullable = true)\n |-- jobStartDate: string (nullable = true)\n |-- isGraduated: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: double (nullable = true)\n\n+---------+---+------------+-----------+------+------+\n|firstname|age|jobStartDate|isGraduated|gender|salary|\n+---------+---+------------+-----------+------+------+\n|James    |34 |2006-01-01  |true       |M     |3000.6|\n|Michael  |33 |1980-01-10  |true       |F     |3300.8|\n|Robert   |37 |06-01-1992  |false      |M     |5000.5|\n+---------+---+------------+-----------+------+------+\n\nroot\n |-- firstname: string (nullable = true)\n |-- age: string (nullable = true)\n |-- jobStartDate: date (nullable = true)\n |-- isGraduated: boolean (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: double (nullable = true)\n\nroot\n |-- age: integer (nullable = true)\n |-- isGraduated: string (nullable = true)\n |-- jobStartDate: string (nullable = true)\n\n+---+-----------+------------+\n|age|isGraduated|jobStartDate|\n+---+-----------+------------+\n|34 |true       |2006-01-01  |\n|33 |true       |1980-01-10  |\n|37 |false      |null        |\n+---+-----------+------------+\n\nroot\n |-- age: string (nullable = true)\n |-- isGraduated: boolean (nullable = true)\n |-- jobStartDate: date (nullable = true)\n\n+---+-----------+------------+\n|age|isGraduated|jobStartDate|\n+---+-----------+------------+\n|34 |true       |2006-01-01  |\n|33 |true       |1980-01-10  |\n|37 |false      |null        |\n+---+-----------+------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nfrom pyspark.sql.types import DoubleType, IntegerType\n# Create SparkSession\nspark = SparkSession.builder \\\n          .appName('SparkByExamples.com') \\\n          .getOrCreate()\n\nsimpleData = [(\"James\",\"34\",\"true\",\"M\",\"3000.6089\"),\n    (\"Michael\",\"33\",\"true\",\"F\",\"3300.8067\"),\n    (\"Robert\",\"37\",\"false\",\"M\",\"5000.5034\")\n  ]\n\ncolumns = [\"firstname\",\"age\",\"isGraduated\",\"gender\",\"salary\"]\ndf = spark.createDataFrame(data = simpleData, schema = columns)\ndf.printSchema()\ndf.show(truncate=False)\n\nfrom pyspark.sql.functions import col,round,expr\ndf.withColumn(\"salary\",df.salary.cast('double')).printSchema()    \ndf.withColumn(\"salary\",df.salary.cast(DoublerType())).printSchema()    \ndf.withColumn(\"salary\",col(\"salary\").cast('double')).printSchema()    \n\n#df.withColumn(\"salary\",round(df.salary.cast(DoubleType()),2)).show(truncate=False).printSchema()    \ndf.selectExpr(\"firstname\",\"isGraduated\",\"cast(salary as double) salary\").printSchema()    \n\ndf.createOrReplaceTempView(\"CastExample\")\nspark.sql(\"SELECT firstname,isGraduated,DOUBLE(salary) as salary from CastExample\").printSchema()\n\n\n#df.select(\"firstname\",expr(df.age),\"isGraduated\",col(\"salary\").cast('float').alias(\"salary\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"1e3d9406-3aab-44b5-9a7a-195559de39e1","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- firstname: string (nullable = true)\n |-- age: string (nullable = true)\n |-- isGraduated: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: string (nullable = true)\n\n+---------+---+-----------+------+---------+\n|firstname|age|isGraduated|gender|salary   |\n+---------+---+-----------+------+---------+\n|James    |34 |true       |M     |3000.6089|\n|Michael  |33 |true       |F     |3300.8067|\n|Robert   |37 |false      |M     |5000.5034|\n+---------+---+-----------+------+---------+\n\nroot\n |-- firstname: string (nullable = true)\n |-- age: string (nullable = true)\n |-- isGraduated: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: double (nullable = true)\n\n"]},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-3712906355083915>:20\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunctions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m col,\u001B[38;5;28mround\u001B[39m,expr\n\u001B[1;32m     19\u001B[0m df\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msalary\u001B[39m\u001B[38;5;124m\"\u001B[39m,df\u001B[38;5;241m.\u001B[39msalary\u001B[38;5;241m.\u001B[39mcast(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdouble\u001B[39m\u001B[38;5;124m'\u001B[39m))\u001B[38;5;241m.\u001B[39mprintSchema()    \n\u001B[0;32m---> 20\u001B[0m df\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msalary\u001B[39m\u001B[38;5;124m\"\u001B[39m,df\u001B[38;5;241m.\u001B[39msalary\u001B[38;5;241m.\u001B[39mcast(DoublerType()))\u001B[38;5;241m.\u001B[39mprintSchema()    \n\u001B[1;32m     21\u001B[0m df\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msalary\u001B[39m\u001B[38;5;124m\"\u001B[39m,col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msalary\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mcast(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdouble\u001B[39m\u001B[38;5;124m'\u001B[39m))\u001B[38;5;241m.\u001B[39mprintSchema()    \n\u001B[1;32m     23\u001B[0m \u001B[38;5;66;03m#df.withColumn(\"salary\",round(df.salary.cast(DoubleType()),2)).show(truncate=False).printSchema()    \u001B[39;00m\n\n\u001B[0;31mNameError\u001B[0m: name 'DoublerType' is not defined","errorSummary":"<span class='ansi-red-fg'>NameError</span>: name 'DoublerType' is not defined","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n","\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n","File \u001B[0;32m<command-3712906355083915>:20\u001B[0m\n","\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunctions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m col,\u001B[38;5;28mround\u001B[39m,expr\n","\u001B[1;32m     19\u001B[0m df\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msalary\u001B[39m\u001B[38;5;124m\"\u001B[39m,df\u001B[38;5;241m.\u001B[39msalary\u001B[38;5;241m.\u001B[39mcast(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdouble\u001B[39m\u001B[38;5;124m'\u001B[39m))\u001B[38;5;241m.\u001B[39mprintSchema()    \n","\u001B[0;32m---> 20\u001B[0m df\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msalary\u001B[39m\u001B[38;5;124m\"\u001B[39m,df\u001B[38;5;241m.\u001B[39msalary\u001B[38;5;241m.\u001B[39mcast(DoublerType()))\u001B[38;5;241m.\u001B[39mprintSchema()    \n","\u001B[1;32m     21\u001B[0m df\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msalary\u001B[39m\u001B[38;5;124m\"\u001B[39m,col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msalary\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mcast(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdouble\u001B[39m\u001B[38;5;124m'\u001B[39m))\u001B[38;5;241m.\u001B[39mprintSchema()    \n","\u001B[1;32m     23\u001B[0m \u001B[38;5;66;03m#df.withColumn(\"salary\",round(df.salary.cast(DoubleType()),2)).show(truncate=False).printSchema()    \u001B[39;00m\n","\n","\u001B[0;31mNameError\u001B[0m: name 'DoublerType' is not defined"]}}],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\ndept = [(\"Finance\",10), \\\n    (\"Marketing\",20), \\\n    (\"Sales\",30), \\\n    (\"IT\",40) \\\n  ]\ndeptColumns = [\"dept_name\",\"dept_id\"]\ndeptDF = spark.createDataFrame(data=dept, schema = deptColumns)\ndeptDF.printSchema()\ndeptDF.show(truncate=False)\n\ndataCollect = deptDF.collect()\n\nprint(dataCollect)\n\ndataCollect2 = deptDF.select(\"dept_name\").collect()\nprint(dataCollect2)\n\nfor row in dataCollect:\n    print(row['dept_name'] + \",\" +str(row['dept_id']))\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"5256c502-3865-4a17-b220-e392e1c390d9","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- dept_name: string (nullable = true)\n |-- dept_id: long (nullable = true)\n\n+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|Finance  |10     |\n|Marketing|20     |\n|Sales    |30     |\n|IT       |40     |\n+---------+-------+\n\n[Row(dept_name='Finance', dept_id=10), Row(dept_name='Marketing', dept_id=20), Row(dept_name='Sales', dept_id=30), Row(dept_name='IT', dept_id=40)]\n[Row(dept_name='Finance'), Row(dept_name='Marketing'), Row(dept_name='Sales'), Row(dept_name='IT')]\nFinance,10\nMarketing,20\nSales,30\nIT,40\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\ndata=[(\"James\",\"Bond\",\"100\",None),\n      (\"Ann\",\"Varsa\",\"200\",'F'),\n      (\"Tom Cruise\",\"XXX\",\"400\",''),\n      (\"Tom Brand\",None,\"400\",'M')] \ncolumns=[\"fname\",\"lname\",\"id\",\"gender\"]\ndf=spark.createDataFrame(data,columns)\n\n#alias\nfrom pyspark.sql.functions import expr\ndf.select(df.fname.alias(\"first_name\"), \\\n          df.lname.alias(\"last_name\"), \\\n          expr(\" fname ||','|| lname\").alias(\"fullName\") \\\n   ).show()\n\n#asc, desc\ndf.sort(df.fname.asc()).show()\ndf.sort(df.fname.desc()).show()\n\n#cast\ndf.select(df.fname,df.id.cast(\"int\")).printSchema()\n\n#between\ndf.filter(df.id.between(100,300)).show()\n\n#contains\ndf.filter(df.fname.contains(\"Cruise\")).show()\n\n#startswith, endswith()\ndf.filter(df.fname.startswith(\"T\")).show()\ndf.filter(df.fname.endswith(\"Cruise\")).show()\n\n#eqNullSafe\n\n#isNull & isNotNull\ndf.filter(df.lname.isNull()).show()\ndf.filter(df.lname.isNotNull()).show()\n\n#like , rlike\ndf.select(df.fname,df.lname,df.id) \\\n  .filter(df.fname.like(\"%om\")) \n\n#over\n\n#substr\ndf.select(df.fname.substr(1,2).alias(\"substr\")).show()\n\n#when & otherwise\nfrom pyspark.sql.functions import when\ndf.select(df.fname,df.lname,when(df.gender==\"M\",\"Male\") \\\n              .when(df.gender==\"F\",\"Female\") \\\n              .when(df.gender==None ,\"\") \\\n              .otherwise(df.gender).alias(\"new_gender\") \\\n    ).show()\n\n#isin\nli=[\"100\",\"200\"]\ndf.select(df.fname,df.lname,df.id) \\\n  .filter(df.id.isin(li)) \\\n  .show()\n\nfrom pyspark.sql.types import StructType,StructField,StringType,ArrayType,MapType\ndata=[((\"James\",\"Bond\"),[\"Java\",\"C#\"],{'hair':'black','eye':'brown'}),\n      ((\"Ann\",\"Varsa\"),[\".NET\",\"Python\"],{'hair':'brown','eye':'black'}),\n      ((\"Tom Cruise\",\"\"),[\"Python\",\"Scala\"],{'hair':'red','eye':'grey'}),\n      ((\"Tom Brand\",None),[\"Perl\",\"Ruby\"],{'hair':'black','eye':'blue'})]\n\nschema = StructType([\n        StructField('name', StructType([\n            StructField('fname', StringType(), True),\n            StructField('lname', StringType(), True)])),\n        StructField('languages', ArrayType(StringType()),True),\n        StructField('properties', MapType(StringType(),StringType()),True)\n     ])\ndf=spark.createDataFrame(data,schema)\ndf.printSchema()\n#getItem()\ndf.select(df.languages.getItem(1)).show()\n\ndf.select(df.properties.getItem(\"hair\")).show()\n\n#getField from Struct or Map\ndf.select(df.properties.getField(\"hair\")).show()\n\ndf.select(df.name.getField(\"fname\")).show()\n\n#dropFields\n#from pyspark.sql.functions import col\n#df.withColumn(\"name1\",col(\"name\").dropFields([\"fname\"])).show()\n\n#withField\n#from pyspark.sql.functions import lit\n#df.withColumn(\"name\",df.name.withField(\"fname\",lit(\"AA\"))).show()\n\n#from pyspark.sql import Row\n#from pyspark.sql.functions import lit\n#df = spark.createDataFrame([Row(a=Row(b=1, c=2))])\n#df.withColumn('a', df['a'].withField('b', lit(3))).select('a.b').show()\n        \n#from pyspark.sql import Row\n#from pyspark.sql.functions import col, lit\n#df = spark.createDataFrame([\n#Row(a=Row(b=1, c=2, d=3, e=Row(f=4, g=5, h=6)))])\n#df.withColumn('a', df['a'].dropFields('b')).show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"2c85ece6-b899-4205-8fbd-7d6a1491552d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----------+---------+--------------+\n|first_name|last_name|      fullName|\n+----------+---------+--------------+\n|     James|     Bond|    James,Bond|\n|       Ann|    Varsa|     Ann,Varsa|\n|Tom Cruise|      XXX|Tom Cruise,XXX|\n| Tom Brand|     null|          null|\n+----------+---------+--------------+\n\n+----------+-----+---+------+\n|     fname|lname| id|gender|\n+----------+-----+---+------+\n|       Ann|Varsa|200|     F|\n|     James| Bond|100|  null|\n| Tom Brand| null|400|     M|\n|Tom Cruise|  XXX|400|      |\n+----------+-----+---+------+\n\n+----------+-----+---+------+\n|     fname|lname| id|gender|\n+----------+-----+---+------+\n|Tom Cruise|  XXX|400|      |\n| Tom Brand| null|400|     M|\n|     James| Bond|100|  null|\n|       Ann|Varsa|200|     F|\n+----------+-----+---+------+\n\nroot\n |-- fname: string (nullable = true)\n |-- id: integer (nullable = true)\n\n+-----+-----+---+------+\n|fname|lname| id|gender|\n+-----+-----+---+------+\n|James| Bond|100|  null|\n|  Ann|Varsa|200|     F|\n+-----+-----+---+------+\n\n+----------+-----+---+------+\n|     fname|lname| id|gender|\n+----------+-----+---+------+\n|Tom Cruise|  XXX|400|      |\n+----------+-----+---+------+\n\n+----------+-----+---+------+\n|     fname|lname| id|gender|\n+----------+-----+---+------+\n|Tom Cruise|  XXX|400|      |\n| Tom Brand| null|400|     M|\n+----------+-----+---+------+\n\n+----------+-----+---+------+\n|     fname|lname| id|gender|\n+----------+-----+---+------+\n|Tom Cruise|  XXX|400|      |\n+----------+-----+---+------+\n\n+---------+-----+---+------+\n|    fname|lname| id|gender|\n+---------+-----+---+------+\n|Tom Brand| null|400|     M|\n+---------+-----+---+------+\n\n+----------+-----+---+------+\n|     fname|lname| id|gender|\n+----------+-----+---+------+\n|     James| Bond|100|  null|\n|       Ann|Varsa|200|     F|\n|Tom Cruise|  XXX|400|      |\n+----------+-----+---+------+\n\n+------+\n|substr|\n+------+\n|    Ja|\n|    An|\n|    To|\n|    To|\n+------+\n\n+----------+-----+----------+\n|     fname|lname|new_gender|\n+----------+-----+----------+\n|     James| Bond|      null|\n|       Ann|Varsa|    Female|\n|Tom Cruise|  XXX|          |\n| Tom Brand| null|      Male|\n+----------+-----+----------+\n\n+-----+-----+---+\n|fname|lname| id|\n+-----+-----+---+\n|James| Bond|100|\n|  Ann|Varsa|200|\n+-----+-----+---+\n\nroot\n |-- name: struct (nullable = true)\n |    |-- fname: string (nullable = true)\n |    |-- lname: string (nullable = true)\n |-- languages: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- properties: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n\n+------------+\n|languages[1]|\n+------------+\n|          C#|\n|      Python|\n|       Scala|\n|        Ruby|\n+------------+\n\n+----------------+\n|properties[hair]|\n+----------------+\n|           black|\n|           brown|\n|             red|\n|           black|\n+----------------+\n\n+----------------+\n|properties[hair]|\n+----------------+\n|           black|\n|           brown|\n|             red|\n|           black|\n+----------------+\n\n+----------+\n|name.fname|\n+----------+\n|     James|\n|       Ann|\n|Tom Cruise|\n| Tom Brand|\n+----------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession,Row\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\ndata=[(\"James\",23),(\"Ann\",40)]\ndf=spark.createDataFrame(data).toDF(\"name.fname\",\"gender\")\ndf.printSchema()\ndf.show()\n\nfrom pyspark.sql.functions import col\ndf.select(col(\"`name.fname`\")).show()\ndf.select(df[\"`name.fname`\"]).show()\ndf.withColumn(\"new_col\",col(\"`name.fname`\").substr(1,2)).show()\ndf.filter(col(\"`name.fname`\").startswith(\"J\")).show()\nnew_cols=(column.replace('.', '_') for column in df.columns)\ndf2 = df.toDF(*new_cols)\ndf2.show()\n\n\n# Using DataFrame object\ndf.select(df.gender).show()\ndf.select(df[\"gender\"]).show()\n#Accessing column name with dot (with backticks)\ndf.select(df[\"`name.fname`\"]).show()\n\n#Using SQL col() function\nfrom pyspark.sql.functions import col\ndf.select(col(\"gender\")).show()\n#Accessing column name with dot (with backticks)\ndf.select(col(\"`name.fname`\")).show()\n\n#Access struct column\ndata=[Row(name=\"James\",prop=Row(hair=\"black\",eye=\"blue\")),\n      Row(name=\"Ann\",prop=Row(hair=\"grey\",eye=\"black\"))]\ndf=spark.createDataFrame(data)\ndf.printSchema()\n\ndf.select(df.prop.hair).show()\ndf.select(df[\"prop.hair\"]).show()\ndf.select(col(\"prop.hair\")).show()\ndf.select(col(\"prop.*\")).show()\n\n# Column operators\ndata=[(100,2,1),(200,3,4),(300,4,4)]\ndf=spark.createDataFrame(data).toDF(\"col1\",\"col2\",\"col3\")\ndf.select(df.col1 + df.col2).show()\ndf.select(df.col1 - df.col2).show() \ndf.select(df.col1 * df.col2).show()\ndf.select(df.col1 / df.col2).show()\ndf.select(df.col1 % df.col2).show()\n\ndf.select(df.col2 > df.col3).show()\ndf.select(df.col2 < df.col3).show()\ndf.select(df.col2 == df.col3).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"875658a5-1768-47d1-82ef-1e7ac1d849b6","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- name.fname: string (nullable = true)\n |-- gender: long (nullable = true)\n\n+----------+------+\n|name.fname|gender|\n+----------+------+\n|     James|    23|\n|       Ann|    40|\n+----------+------+\n\n+----------+\n|name.fname|\n+----------+\n|     James|\n|       Ann|\n+----------+\n\n+----------+\n|name.fname|\n+----------+\n|     James|\n|       Ann|\n+----------+\n\n+----------+------+-------+\n|name.fname|gender|new_col|\n+----------+------+-------+\n|     James|    23|     Ja|\n|       Ann|    40|     An|\n+----------+------+-------+\n\n+----------+------+\n|name.fname|gender|\n+----------+------+\n|     James|    23|\n+----------+------+\n\n+----------+------+\n|name_fname|gender|\n+----------+------+\n|     James|    23|\n|       Ann|    40|\n+----------+------+\n\n+------+\n|gender|\n+------+\n|    23|\n|    40|\n+------+\n\n+------+\n|gender|\n+------+\n|    23|\n|    40|\n+------+\n\n+----------+\n|name.fname|\n+----------+\n|     James|\n|       Ann|\n+----------+\n\n+------+\n|gender|\n+------+\n|    23|\n|    40|\n+------+\n\n+----------+\n|name.fname|\n+----------+\n|     James|\n|       Ann|\n+----------+\n\nroot\n |-- name: string (nullable = true)\n |-- prop: struct (nullable = true)\n |    |-- hair: string (nullable = true)\n |    |-- eye: string (nullable = true)\n\n+---------+\n|prop.hair|\n+---------+\n|    black|\n|     grey|\n+---------+\n\n+-----+\n| hair|\n+-----+\n|black|\n| grey|\n+-----+\n\n+-----+\n| hair|\n+-----+\n|black|\n| grey|\n+-----+\n\n+-----+-----+\n| hair|  eye|\n+-----+-----+\n|black| blue|\n| grey|black|\n+-----+-----+\n\n+-------------+\n|(col1 + col2)|\n+-------------+\n|          102|\n|          203|\n|          304|\n+-------------+\n\n+-------------+\n|(col1 - col2)|\n+-------------+\n|           98|\n|          197|\n|          296|\n+-------------+\n\n+-------------+\n|(col1 * col2)|\n+-------------+\n|          200|\n|          600|\n|         1200|\n+-------------+\n\n+-----------------+\n|    (col1 / col2)|\n+-----------------+\n|             50.0|\n|66.66666666666667|\n|             75.0|\n+-----------------+\n\n+-------------+\n|(col1 % col2)|\n+-------------+\n|            0|\n|            2|\n|            0|\n+-------------+\n\n+-------------+\n|(col2 > col3)|\n+-------------+\n|         true|\n|        false|\n|        false|\n+-------------+\n\n+-------------+\n|(col2 < col3)|\n+-------------+\n|        false|\n|         true|\n|        false|\n+-------------+\n\n+-------------+\n|(col2 = col3)|\n+-------------+\n|        false|\n|        false|\n|         true|\n+-------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\ndataDictionary = [\n        ('James',{'hair':'black','eye':'brown'}),\n        ('Michael',{'hair':'brown','eye':None}),\n        ('Robert',{'hair':'red','eye':'black'}),\n        ('Washington',{'hair':'grey','eye':'grey'}),\n        ('Jefferson',{'hair':'brown','eye':''})\n        ]\n\ndf = spark.createDataFrame(data=dataDictionary, schema = ['name','properties'])\ndf.printSchema()\ndf.show(truncate=False)\n\ndf3=df.rdd.map(lambda x: \\\n    (x.name,x.properties[\"hair\"],x.properties[\"eye\"])) \\\n    .toDF([\"name\",\"hair\",\"eye\"])\ndf3.printSchema()\ndf3.show()\n\ndf.withColumn(\"hair\",df.properties.getItem(\"hair\")) \\\n  .withColumn(\"eye\",df.properties.getItem(\"eye\")) \\\n  .drop(\"properties\") \\\n  .show()\n\ndf.withColumn(\"hair\",df.properties[\"hair\"]) \\\n  .withColumn(\"eye\",df.properties[\"eye\"]) \\\n  .drop(\"properties\") \\\n  .show()\n\n# Functions\nfrom pyspark.sql.functions import explode,map_keys,col\nkeysDF = df.select(explode(map_keys(df.properties))).distinct()\nkeysList = keysDF.rdd.map(lambda x:x[0]).collect()\nkeyCols = list(map(lambda x: col(\"properties\").getItem(x).alias(str(x)), keysList))\ndf.select(df.name, *keyCols).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"3de7e408-16cb-41e3-a768-398475d4bc17","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- name: string (nullable = true)\n |-- properties: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n\n+----------+-----------------------------+\n|name      |properties                   |\n+----------+-----------------------------+\n|James     |{eye -> brown, hair -> black}|\n|Michael   |{eye -> null, hair -> brown} |\n|Robert    |{eye -> black, hair -> red}  |\n|Washington|{eye -> grey, hair -> grey}  |\n|Jefferson |{eye -> , hair -> brown}     |\n+----------+-----------------------------+\n\nroot\n |-- name: string (nullable = true)\n |-- hair: string (nullable = true)\n |-- eye: string (nullable = true)\n\n+----------+-----+-----+\n|      name| hair|  eye|\n+----------+-----+-----+\n|     James|black|brown|\n|   Michael|brown| null|\n|    Robert|  red|black|\n|Washington| grey| grey|\n| Jefferson|brown|     |\n+----------+-----+-----+\n\n+----------+-----+-----+\n|      name| hair|  eye|\n+----------+-----+-----+\n|     James|black|brown|\n|   Michael|brown| null|\n|    Robert|  red|black|\n|Washington| grey| grey|\n| Jefferson|brown|     |\n+----------+-----+-----+\n\n+----------+-----+-----+\n|      name| hair|  eye|\n+----------+-----+-----+\n|     James|black|brown|\n|   Michael|brown| null|\n|    Robert|  red|black|\n|Washington| grey| grey|\n| Jefferson|brown|     |\n+----------+-----+-----+\n\n+----------+-----+-----+\n|      name|  eye| hair|\n+----------+-----+-----+\n|     James|brown|black|\n|   Michael| null|brown|\n|    Robert|black|  red|\n|Washington| grey| grey|\n| Jefferson|     |brown|\n+----------+-----+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\ndata = [ (\"36636\",\"Finance\",3000,\"USA\"), \n    (\"40288\",\"Finance\",5000,\"IND\"), \n    (\"42114\",\"Sales\",3900,\"USA\"), \n    (\"39192\",\"Marketing\",2500,\"CAN\"), \n    (\"34534\",\"Sales\",6500,\"USA\") ]\nschema = StructType([\n     StructField('id', StringType(), True),\n     StructField('dept', StringType(), True),\n     StructField('salary', IntegerType(), True),\n     StructField('location', StringType(), True)\n     ])\n\ndf = spark.createDataFrame(data=data,schema=schema)\ndf.printSchema()\ndf.show(truncate=False)\n\n#Convert scolumns to Map\nfrom pyspark.sql.functions import col,lit,create_map\ndf = df.withColumn(\"propertiesMap\",create_map(\n        lit(\"salary\"),col(\"salary\"),\n        lit(\"location\"),col(\"location\")\n        )).drop(\"salary\",\"location\")\ndf.printSchema()\ndf.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"4c7b1f26-181a-4615-8454-162539884199","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- id: string (nullable = true)\n |-- dept: string (nullable = true)\n |-- salary: integer (nullable = true)\n |-- location: string (nullable = true)\n\n+-----+---------+------+--------+\n|id   |dept     |salary|location|\n+-----+---------+------+--------+\n|36636|Finance  |3000  |USA     |\n|40288|Finance  |5000  |IND     |\n|42114|Sales    |3900  |USA     |\n|39192|Marketing|2500  |CAN     |\n|34534|Sales    |6500  |USA     |\n+-----+---------+------+--------+\n\nroot\n |-- id: string (nullable = true)\n |-- dept: string (nullable = true)\n |-- propertiesMap: map (nullable = false)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n\n+-----+---------+---------------------------------+\n|id   |dept     |propertiesMap                    |\n+-----+---------+---------------------------------+\n|36636|Finance  |{salary -> 3000, location -> USA}|\n|40288|Finance  |{salary -> 5000, location -> IND}|\n|42114|Sales    |{salary -> 3900, location -> USA}|\n|39192|Marketing|{salary -> 2500, location -> CAN}|\n|34534|Sales    |{salary -> 6500, location -> USA}|\n+-----+---------+---------------------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder \\\n         .appName('SparkByExamples.com') \\\n         .getOrCreate()\n\ndata = [(\"James\", \"Sales\", 3000),\n    (\"Michael\", \"Sales\", 4600),\n    (\"Robert\", \"Sales\", 4100),\n    (\"Maria\", \"Finance\", 3000),\n    (\"James\", \"Sales\", 3000),\n    (\"Scott\", \"Finance\", 3300),\n    (\"Jen\", \"Finance\", 3900),\n    (\"Jeff\", \"Marketing\", 3000),\n    (\"Kumar\", \"Marketing\", 2000),\n    (\"Saif\", \"Sales\", 4100)\n  ]\ncolumns = [\"Name\",\"Dept\",\"Salary\"]\ndf = spark.createDataFrame(data=data,schema=columns)\ndf.distinct().show()\nprint(\"Distinct Count: \" + str(df.distinct().count()))\n\n# Using countDistrinct()\nfrom pyspark.sql.functions import countDistinct\ndf2=df.select(countDistinct(\"Dept\",\"Salary\"))\ndf2.show()\n\nprint(\"Distinct Count of Department &amp; Salary: \"+ str(df2.collect()[0][0]))\n\ndf.createOrReplaceTempView(\"PERSON\")\nspark.sql(\"select distinct(count(*)) from PERSON\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"e76331c5-918b-4579-aa6a-58e42b13df98","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------+---------+------+\n|   Name|     Dept|Salary|\n+-------+---------+------+\n|  James|    Sales|  3000|\n|Michael|    Sales|  4600|\n| Robert|    Sales|  4100|\n|  Maria|  Finance|  3000|\n|  Scott|  Finance|  3300|\n|    Jen|  Finance|  3900|\n|   Jeff|Marketing|  3000|\n|  Kumar|Marketing|  2000|\n|   Saif|    Sales|  4100|\n+-------+---------+------+\n\nDistinct Count: 9\n+----------------------------+\n|count(DISTINCT Dept, Salary)|\n+----------------------------+\n|                           8|\n+----------------------------+\n\nDistinct Count of Department &amp; Salary: 8\n+--------+\n|count(1)|\n+--------+\n|      10|\n+--------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\ndataDictionary = [\n        ('James',{'hair':'black','eye':'brown'}),\n        ('Michael',{'hair':'brown','eye':None}),\n        ('Robert',{'hair':'red','eye':'black'}),\n        ('Washington',{'hair':'grey','eye':'grey'}),\n        ('Jefferson',{'hair':'brown','eye':''})\n        ]\n\ndf = spark.createDataFrame(data=dataDictionary, schema = ['name','properties'])\ndf.printSchema()\ndf.show(truncate=False)\n\n# Using StructType schema\nfrom pyspark.sql.types import StructField, StructType, StringType, MapType,IntegerType\nschema = StructType([\n    StructField('name', StringType(), True),\n    StructField('properties', MapType(StringType(),StringType()),True)\n])\ndf2 = spark.createDataFrame(data=dataDictionary, schema = schema)\ndf2.printSchema()\ndf2.show(truncate=False)\n\ndf3=df.rdd.map(lambda x: \\\n    (x.name,x.properties[\"hair\"],x.properties[\"eye\"])) \\\n    .toDF([\"name\",\"hair\",\"eye\"])\ndf3.printSchema()\ndf3.show()\n\ndf.withColumn(\"hair\",df.properties.getItem(\"hair\")) \\\n  .withColumn(\"eye\",df.properties.getItem(\"eye\")) \\\n  .drop(\"properties\") \\\n  .show()\n\ndf.withColumn(\"hair\",df.properties[\"hair\"]) \\\n  .withColumn(\"eye\",df.properties[\"eye\"]) \\\n  .drop(\"properties\") \\\n  .show()\n\n# Functions\nfrom pyspark.sql.functions import explode,map_keys,col\nkeysDF = df.select(explode(map_keys(df.properties))).distinct()\nkeysList = keysDF.rdd.map(lambda x:x[0]).collect()\nkeyCols = list(map(lambda x: col(\"properties\").getItem(x).alias(str(x)), keysList))\ndf.select(df.name, *keyCols).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"7ad0b28a-21d5-481d-b6ce-7bd42c6d7162","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- name: string (nullable = true)\n |-- properties: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n\n+----------+-----------------------------+\n|name      |properties                   |\n+----------+-----------------------------+\n|James     |{eye -> brown, hair -> black}|\n|Michael   |{eye -> null, hair -> brown} |\n|Robert    |{eye -> black, hair -> red}  |\n|Washington|{eye -> grey, hair -> grey}  |\n|Jefferson |{eye -> , hair -> brown}     |\n+----------+-----------------------------+\n\nroot\n |-- name: string (nullable = true)\n |-- properties: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n\n+----------+-----------------------------+\n|name      |properties                   |\n+----------+-----------------------------+\n|James     |{eye -> brown, hair -> black}|\n|Michael   |{eye -> null, hair -> brown} |\n|Robert    |{eye -> black, hair -> red}  |\n|Washington|{eye -> grey, hair -> grey}  |\n|Jefferson |{eye -> , hair -> brown}     |\n+----------+-----------------------------+\n\nroot\n |-- name: string (nullable = true)\n |-- hair: string (nullable = true)\n |-- eye: string (nullable = true)\n\n+----------+-----+-----+\n|      name| hair|  eye|\n+----------+-----+-----+\n|     James|black|brown|\n|   Michael|brown| null|\n|    Robert|  red|black|\n|Washington| grey| grey|\n| Jefferson|brown|     |\n+----------+-----+-----+\n\n+----------+-----+-----+\n|      name| hair|  eye|\n+----------+-----+-----+\n|     James|black|brown|\n|   Michael|brown| null|\n|    Robert|  red|black|\n|Washington| grey| grey|\n| Jefferson|brown|     |\n+----------+-----+-----+\n\n+----------+-----+-----+\n|      name| hair|  eye|\n+----------+-----+-----+\n|     James|black|brown|\n|   Michael|brown| null|\n|    Robert|  red|black|\n|Washington| grey| grey|\n| Jefferson|brown|     |\n+----------+-----+-----+\n\n+----------+-----+-----+\n|      name|  eye| hair|\n+----------+-----+-----+\n|     James|brown|black|\n|   Michael| null|brown|\n|    Robert|black|  red|\n|Washington| grey| grey|\n| Jefferson|     |brown|\n+----------+-----+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession, Row\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType\nfrom pyspark.sql.functions import *\n\ncolumns = [\"language\",\"users_count\"]\ndata = [(\"Java\", \"20000\"), (\"Python\", \"100000\"), (\"Scala\", \"3000\")]\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\nrdd = spark.sparkContext.parallelize(data)\n\ndfFromRDD1 = rdd.toDF()\ndfFromRDD1.printSchema()\n\ndfFromRDD1 = rdd.toDF(columns)\ndfFromRDD1.printSchema()\n\ndfFromRDD2 = spark.createDataFrame(rdd).toDF(*columns)\ndfFromRDD2.printSchema()\n\ndfFromData2 = spark.createDataFrame(data).toDF(*columns)\ndfFromData2.printSchema()     \n\nrowData = map(lambda x: Row(*x), data) \ndfFromData3 = spark.createDataFrame(rowData,columns)\ndfFromData3.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"5fbc71b6-1255-43bd-a215-46e98da8e93a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- _1: string (nullable = true)\n |-- _2: string (nullable = true)\n\nroot\n |-- language: string (nullable = true)\n |-- users_count: string (nullable = true)\n\nroot\n |-- language: string (nullable = true)\n |-- users_count: string (nullable = true)\n\nroot\n |-- language: string (nullable = true)\n |-- users_count: string (nullable = true)\n\nroot\n |-- language: string (nullable = true)\n |-- users_count: string (nullable = true)\n\n"]}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Untitled Notebook 2023-06-12 22:25:10","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
