{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\n\nspark: SparkSession = SparkSession.builder \\\n    .master(\"local[1]\") \\\n    .appName(\"SparkByExamples.com\") \\\n    .getOrCreate()\n\nfilePath=\"resources/small_zipcode.csv\"\ndf = spark.read.options(header='true', inferSchema='true') \\\n          .csv(filePath)\n\ndf.printSchema()\ndf.show(truncate=False)\n\ndf.na.drop().show(truncate=False)\n\ndf.na.drop(how=\"any\").show(truncate=False)\n\ndf.na.drop(subset=[\"population\",\"type\"]) \\\n   .show(truncate=False)\n\ndf.dropna().show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"8111e39f-d125-4bdb-8fae-66eba6097506","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mIllegalArgumentException\u001B[0m                  Traceback (most recent call last)\nFile \u001B[0;32m<command-3712906355083938>:9\u001B[0m\n\u001B[1;32m      3\u001B[0m spark: SparkSession \u001B[38;5;241m=\u001B[39m SparkSession\u001B[38;5;241m.\u001B[39mbuilder \\\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;241m.\u001B[39mmaster(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlocal[1]\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;241m.\u001B[39mappName(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSparkByExamples.com\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;241m.\u001B[39mgetOrCreate()\n\u001B[1;32m      8\u001B[0m filePath\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mresources/small_zipcode.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 9\u001B[0m df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39moptions(header\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m'\u001B[39m, inferSchema\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m'\u001B[39m) \\\n\u001B[1;32m     10\u001B[0m           \u001B[38;5;241m.\u001B[39mcsv(filePath)\n\u001B[1;32m     12\u001B[0m df\u001B[38;5;241m.\u001B[39mprintSchema()\n\u001B[1;32m     13\u001B[0m df\u001B[38;5;241m.\u001B[39mshow(truncate\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:729\u001B[0m, in \u001B[0;36mDataFrameReader.csv\u001B[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001B[0m\n\u001B[1;32m    727\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(path) \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mlist\u001B[39m:\n\u001B[1;32m    728\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_spark\u001B[38;5;241m.\u001B[39m_sc\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 729\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_df(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jreader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcsv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_spark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jvm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mPythonUtils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtoSeq\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    730\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(path, RDD):\n\u001B[1;32m    732\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfunc\u001B[39m(iterator):\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mIllegalArgumentException\u001B[0m: Path must be absolute: resources/small_zipcode.csv","errorSummary":"<span class='ansi-red-fg'>IllegalArgumentException</span>: Path must be absolute: resources/small_zipcode.csv","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n","\u001B[0;31mIllegalArgumentException\u001B[0m                  Traceback (most recent call last)\n","File \u001B[0;32m<command-3712906355083938>:9\u001B[0m\n","\u001B[1;32m      3\u001B[0m spark: SparkSession \u001B[38;5;241m=\u001B[39m SparkSession\u001B[38;5;241m.\u001B[39mbuilder \\\n","\u001B[1;32m      4\u001B[0m     \u001B[38;5;241m.\u001B[39mmaster(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlocal[1]\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n","\u001B[1;32m      5\u001B[0m     \u001B[38;5;241m.\u001B[39mappName(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSparkByExamples.com\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n","\u001B[1;32m      6\u001B[0m     \u001B[38;5;241m.\u001B[39mgetOrCreate()\n","\u001B[1;32m      8\u001B[0m filePath\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mresources/small_zipcode.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m\n","\u001B[0;32m----> 9\u001B[0m df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39moptions(header\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m'\u001B[39m, inferSchema\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m'\u001B[39m) \\\n","\u001B[1;32m     10\u001B[0m           \u001B[38;5;241m.\u001B[39mcsv(filePath)\n","\u001B[1;32m     12\u001B[0m df\u001B[38;5;241m.\u001B[39mprintSchema()\n","\u001B[1;32m     13\u001B[0m df\u001B[38;5;241m.\u001B[39mshow(truncate\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n","\n","File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n","\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n","\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n","\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n","\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n","\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n","\u001B[1;32m     51\u001B[0m     )\n","\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n","\n","File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:729\u001B[0m, in \u001B[0;36mDataFrameReader.csv\u001B[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001B[0m\n","\u001B[1;32m    727\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(path) \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mlist\u001B[39m:\n","\u001B[1;32m    728\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_spark\u001B[38;5;241m.\u001B[39m_sc\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n","\u001B[0;32m--> 729\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_df(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jreader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcsv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_spark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jvm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mPythonUtils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtoSeq\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m)\n","\u001B[1;32m    730\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(path, RDD):\n","\u001B[1;32m    732\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfunc\u001B[39m(iterator):\n","\n","File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n","\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n","\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n","\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n","\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n","\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n","\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n","\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n","\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n","\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n","\n","File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n","\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n","\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n","\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n","\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n","\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n","\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n","\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n","\n","\u001B[0;31mIllegalArgumentException\u001B[0m: Path must be absolute: resources/small_zipcode.csv"]}}],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType,StructField, StringType\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\nschema = StructType([\n  StructField('firstname', StringType(), True),\n  StructField('middlename', StringType(), True),\n  StructField('lastname', StringType(), True)\n  ])\ndf = spark.createDataFrame(spark.sparkContext.emptyRDD(),schema)\ndf.printSchema()\n\ndf1 = spark.sparkContext.parallelize([]).toDF(schema)\ndf1.printSchema()\n\ndf2 = spark.createDataFrame([], schema)\ndf2.printSchema()\n\ndf3 = spark.emptyDataFrame()\ndf3.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"f89e9cb5-c24d-4770-aa52-d77231bbd48e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- firstname: string (nullable = true)\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n\nroot\n |-- firstname: string (nullable = true)\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n\nroot\n |-- firstname: string (nullable = true)\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n\n"]},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\nFile \u001B[0;32m<command-3712906355083939>:21\u001B[0m\n\u001B[1;32m     18\u001B[0m df2 \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mcreateDataFrame([], schema)\n\u001B[1;32m     19\u001B[0m df2\u001B[38;5;241m.\u001B[39mprintSchema()\n\u001B[0;32m---> 21\u001B[0m df3 \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39memptyDataFrame()\n\u001B[1;32m     22\u001B[0m df3\u001B[38;5;241m.\u001B[39mprintSchema()\n\n\u001B[0;31mAttributeError\u001B[0m: 'SparkSession' object has no attribute 'emptyDataFrame'","errorSummary":"<span class='ansi-red-fg'>AttributeError</span>: 'SparkSession' object has no attribute 'emptyDataFrame'","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n","\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\n","File \u001B[0;32m<command-3712906355083939>:21\u001B[0m\n","\u001B[1;32m     18\u001B[0m df2 \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mcreateDataFrame([], schema)\n","\u001B[1;32m     19\u001B[0m df2\u001B[38;5;241m.\u001B[39mprintSchema()\n","\u001B[0;32m---> 21\u001B[0m df3 \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39memptyDataFrame()\n","\u001B[1;32m     22\u001B[0m df3\u001B[38;5;241m.\u001B[39mprintSchema()\n","\n","\u001B[0;31mAttributeError\u001B[0m: 'SparkSession' object has no attribute 'emptyDataFrame'"]}}],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('pyspark-by-examples').getOrCreate()\n\narrayData = [\n        ('James',['Java','Scala'],{'hair':'black','eye':'brown'}),\n        ('Michael',['Spark','Java',None],{'hair':'brown','eye':None}),\n        ('Robert',['CSharp',''],{'hair':'red','eye':''}),\n        ('Washington',None,None),\n        ('Jefferson',['1','2'],{})\n        ]\ndf = spark.createDataFrame(data=arrayData, schema = ['name','knownLanguages','properties'])\ndf.printSchema()\ndf.show()\n\nfrom pyspark.sql.functions import explode\ndf2 = df.select(df.name,explode(df.knownLanguages))\ndf2.printSchema()\ndf2.show()\n\nfrom pyspark.sql.functions import explode\ndf3 = df.select(df.name,explode(df.properties))\ndf3.printSchema()\ndf3.show()\n\nfrom pyspark.sql.functions import explode_outer\n\"\"\" with array \"\"\"\ndf.select(df.name,explode_outer(df.knownLanguages)).show()\n\"\"\" with map \"\"\"\ndf.select(df.name,explode_outer(df.properties)).show()\n\n\nfrom pyspark.sql.functions import posexplode\n\"\"\" with array \"\"\"\ndf.select(df.name,posexplode(df.knownLanguages)).show()\n\"\"\" with map \"\"\"\ndf.select(df.name,posexplode(df.properties)).show()\n\nfrom pyspark.sql.functions import posexplode_outer\n\"\"\" with array \"\"\"\ndf.select(df.name,posexplode_outer(df.knownLanguages)).show()\n\n\"\"\" with map \"\"\"\ndf.select(df.name,posexplode_outer(df.properties)).show()\n\n\n\"\"\"END\"\"\""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"8a3b8e6b-052b-418c-948e-326c4326a79e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- name: string (nullable = true)\n |-- knownLanguages: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- properties: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n\n+----------+-------------------+--------------------+\n|      name|     knownLanguages|          properties|\n+----------+-------------------+--------------------+\n|     James|      [Java, Scala]|{eye -> brown, ha...|\n|   Michael|[Spark, Java, null]|{eye -> null, hai...|\n|    Robert|         [CSharp, ]|{eye -> , hair ->...|\n|Washington|               null|                null|\n| Jefferson|             [1, 2]|                  {}|\n+----------+-------------------+--------------------+\n\nroot\n |-- name: string (nullable = true)\n |-- col: string (nullable = true)\n\n+---------+------+\n|     name|   col|\n+---------+------+\n|    James|  Java|\n|    James| Scala|\n|  Michael| Spark|\n|  Michael|  Java|\n|  Michael|  null|\n|   Robert|CSharp|\n|   Robert|      |\n|Jefferson|     1|\n|Jefferson|     2|\n+---------+------+\n\nroot\n |-- name: string (nullable = true)\n |-- key: string (nullable = false)\n |-- value: string (nullable = true)\n\n+-------+----+-----+\n|   name| key|value|\n+-------+----+-----+\n|  James| eye|brown|\n|  James|hair|black|\n|Michael| eye| null|\n|Michael|hair|brown|\n| Robert| eye|     |\n| Robert|hair|  red|\n+-------+----+-----+\n\n+----------+------+\n|      name|   col|\n+----------+------+\n|     James|  Java|\n|     James| Scala|\n|   Michael| Spark|\n|   Michael|  Java|\n|   Michael|  null|\n|    Robert|CSharp|\n|    Robert|      |\n|Washington|  null|\n| Jefferson|     1|\n| Jefferson|     2|\n+----------+------+\n\n+----------+----+-----+\n|      name| key|value|\n+----------+----+-----+\n|     James| eye|brown|\n|     James|hair|black|\n|   Michael| eye| null|\n|   Michael|hair|brown|\n|    Robert| eye|     |\n|    Robert|hair|  red|\n|Washington|null| null|\n| Jefferson|null| null|\n+----------+----+-----+\n\n+---------+---+------+\n|     name|pos|   col|\n+---------+---+------+\n|    James|  0|  Java|\n|    James|  1| Scala|\n|  Michael|  0| Spark|\n|  Michael|  1|  Java|\n|  Michael|  2|  null|\n|   Robert|  0|CSharp|\n|   Robert|  1|      |\n|Jefferson|  0|     1|\n|Jefferson|  1|     2|\n+---------+---+------+\n\n+-------+---+----+-----+\n|   name|pos| key|value|\n+-------+---+----+-----+\n|  James|  0| eye|brown|\n|  James|  1|hair|black|\n|Michael|  0| eye| null|\n|Michael|  1|hair|brown|\n| Robert|  0| eye|     |\n| Robert|  1|hair|  red|\n+-------+---+----+-----+\n\n+----------+----+------+\n|      name| pos|   col|\n+----------+----+------+\n|     James|   0|  Java|\n|     James|   1| Scala|\n|   Michael|   0| Spark|\n|   Michael|   1|  Java|\n|   Michael|   2|  null|\n|    Robert|   0|CSharp|\n|    Robert|   1|      |\n|Washington|null|  null|\n| Jefferson|   0|     1|\n| Jefferson|   1|     2|\n+----------+----+------+\n\n+----------+----+----+-----+\n|      name| pos| key|value|\n+----------+----+----+-----+\n|     James|   0| eye|brown|\n|     James|   1|hair|black|\n|   Michael|   0| eye| null|\n|   Michael|   1|hair|brown|\n|    Robert|   0| eye|     |\n|    Robert|   1|hair|  red|\n|Washington|null|null| null|\n| Jefferson|null|null| null|\n+----------+----+----+-----+\n\nOut[3]: 'END'"]}],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import explode, flatten\n\n\nspark = SparkSession.builder.appName('pyspark-by-examples').getOrCreate()\n\narrayArrayData = [\n  (\"James\",[[\"Java\",\"Scala\",\"C++\"],[\"Spark\",\"Java\"]]),\n  (\"Michael\",[[\"Spark\",\"Java\",\"C++\"],[\"Spark\",\"Java\"]]),\n  (\"Robert\",[[\"CSharp\",\"VB\"],[\"Spark\",\"Python\"]])\n]\n\ndf = spark.createDataFrame(data=arrayArrayData, schema = ['name','subjects'])\ndf.printSchema()\ndf.show(truncate=False)\n\n\"\"\" \"\"\"\ndf.select(df.name,explode(df.subjects)).show(truncate=False)\n\n\"\"\" creates a single array from an array of arrays. \"\"\"\ndf.select(df.name,flatten(df.subjects)).show(truncate=False)\n\n\"\"\"END\"\"\""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"4f092ee6-eb51-4552-87b6-300c9b67502d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- name: string (nullable = true)\n |-- subjects: array (nullable = true)\n |    |-- element: array (containsNull = true)\n |    |    |-- element: string (containsNull = true)\n\n+-------+-----------------------------------+\n|name   |subjects                           |\n+-------+-----------------------------------+\n|James  |[[Java, Scala, C++], [Spark, Java]]|\n|Michael|[[Spark, Java, C++], [Spark, Java]]|\n|Robert |[[CSharp, VB], [Spark, Python]]    |\n+-------+-----------------------------------+\n\n+-------+------------------+\n|name   |col               |\n+-------+------------------+\n|James  |[Java, Scala, C++]|\n|James  |[Spark, Java]     |\n|Michael|[Spark, Java, C++]|\n|Michael|[Spark, Java]     |\n|Robert |[CSharp, VB]      |\n|Robert |[Spark, Python]   |\n+-------+------------------+\n\n+-------+-------------------------------+\n|name   |flatten(subjects)              |\n+-------+-------------------------------+\n|James  |[Java, Scala, C++, Spark, Java]|\n|Michael|[Spark, Java, C++, Spark, Java]|\n|Robert |[CSharp, VB, Spark, Python]    |\n+-------+-------------------------------+\n\nOut[4]: 'END'"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\nfrom pyspark.sql.functions import expr\n#Concatenate columns\ndata=[(\"James\",\"Bond\"),(\"Scott\",\"Varsa\")] \ndf=spark.createDataFrame(data).toDF(\"col1\",\"col2\") \ndf.withColumn(\"Name\",expr(\" col1 ||','|| col2\")).show()\n\n#Using CASE WHEN sql expression\ndata = [(\"James\",\"M\"),(\"Michael\",\"F\"),(\"Jen\",\"\")]\ncolumns = [\"name\",\"gender\"]\ndf = spark.createDataFrame(data = data, schema = columns)\ndf2 = df.withColumn(\"gender\", expr(\"CASE WHEN gender = 'M' THEN 'Male' \" +\n           \"WHEN gender = 'F' THEN 'Female' ELSE 'unknown' END\"))\ndf2.show()\n\n#Add months from a value of another column\ndata=[(\"2019-01-23\",1),(\"2019-06-24\",2),(\"2019-09-20\",3)] \ndf=spark.createDataFrame(data).toDF(\"date\",\"increment\") \ndf.select(df.date,df.increment,\n     expr(\"add_months(date,increment)\")\n  .alias(\"inc_date\")).show()\n\n# Providing alias using 'as'\ndf.select(df.date,df.increment,\n     expr(\"\"\"add_months(date,increment) as inc_date\"\"\")\n  ).show()\n\n# Add\ndf.select(df.date,df.increment,\n     expr(\"increment + 5 as new_increment\")\n  ).show()\n\ndf.select(\"increment\",expr(\"cast(increment as string) as str_increment\")) \\\n  .printSchema()\n#Use expr()  to filter the rows\ndata=[(100,2),(200,3000),(500,500)] \ndf=spark.createDataFrame(data).toDF(\"col1\",\"col2\") \ndf.filter(expr(\"col1 == col2\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"b7b7e995-9654-43cd-b142-62e1d71fc172","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----+-----+-----------+\n| col1| col2|       Name|\n+-----+-----+-----------+\n|James| Bond| James,Bond|\n|Scott|Varsa|Scott,Varsa|\n+-----+-----+-----------+\n\n+-------+-------+\n|   name| gender|\n+-------+-------+\n|  James|   Male|\n|Michael| Female|\n|    Jen|unknown|\n+-------+-------+\n\n+----------+---------+----------+\n|      date|increment|  inc_date|\n+----------+---------+----------+\n|2019-01-23|        1|2019-02-23|\n|2019-06-24|        2|2019-08-24|\n|2019-09-20|        3|2019-12-20|\n+----------+---------+----------+\n\n+----------+---------+----------+\n|      date|increment|  inc_date|\n+----------+---------+----------+\n|2019-01-23|        1|2019-02-23|\n|2019-06-24|        2|2019-08-24|\n|2019-09-20|        3|2019-12-20|\n+----------+---------+----------+\n\n+----------+---------+-------------+\n|      date|increment|new_increment|\n+----------+---------+-------------+\n|2019-01-23|        1|            6|\n|2019-06-24|        2|            7|\n|2019-09-20|        3|            8|\n+----------+---------+-------------+\n\nroot\n |-- increment: long (nullable = true)\n |-- str_increment: string (nullable = true)\n\n+----+----+\n|col1|col2|\n+----+----+\n| 500| 500|\n+----+----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\nspark: SparkSession = SparkSession.builder \\\n    .master(\"local[1]\") \\\n    .appName(\"SparkByExamples.com\") \\\n    .getOrCreate()\n\ndata = [\n    (\"James\",None,\"M\"),\n    (\"Anna\",\"NY\",\"F\"),\n    (\"Julia\",None,None)\n]\n\ncolumns = [\"name\",\"state\",\"gender\"]\ndf =spark.createDataFrame(data,columns)\n\ndf.printSchema()\ndf.show()\n\ndf.filter(\"state is NULL\").show()\ndf.filter(df.state.isNull()).show()\ndf.filter(col(\"state\").isNull()).show()\n\ndf.filter(\"state IS NULL AND gender IS NULL\").show()\ndf.filter(df.state.isNull() & df.gender.isNull()).show()\n\ndf.filter(\"state is not NULL\").show()\ndf.filter(\"NOT state is NULL\").show()\ndf.filter(df.state.isNotNull()).show()\ndf.filter(col(\"state\").isNotNull()).show()\ndf.na.drop(subset=[\"state\"]).show()\n\ndf.createOrReplaceTempView(\"DATA\")\nspark.sql(\"SELECT * FROM DATA where STATE IS NULL\").show()\nspark.sql(\"SELECT * FROM DATA where STATE IS NULL AND GENDER IS NULL\").show()\nspark.sql(\"SELECT * FROM DATA where STATE IS NOT NULL\").show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"a09aa457-92e1-46c3-95a7-7154f9eb992e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- name: string (nullable = true)\n |-- state: string (nullable = true)\n |-- gender: string (nullable = true)\n\n+-----+-----+------+\n| name|state|gender|\n+-----+-----+------+\n|James| null|     M|\n| Anna|   NY|     F|\n|Julia| null|  null|\n+-----+-----+------+\n\n+-----+-----+------+\n| name|state|gender|\n+-----+-----+------+\n|James| null|     M|\n|Julia| null|  null|\n+-----+-----+------+\n\n+-----+-----+------+\n| name|state|gender|\n+-----+-----+------+\n|James| null|     M|\n|Julia| null|  null|\n+-----+-----+------+\n\n+-----+-----+------+\n| name|state|gender|\n+-----+-----+------+\n|James| null|     M|\n|Julia| null|  null|\n+-----+-----+------+\n\n+-----+-----+------+\n| name|state|gender|\n+-----+-----+------+\n|Julia| null|  null|\n+-----+-----+------+\n\n+-----+-----+------+\n| name|state|gender|\n+-----+-----+------+\n|Julia| null|  null|\n+-----+-----+------+\n\n+----+-----+------+\n|name|state|gender|\n+----+-----+------+\n|Anna|   NY|     F|\n+----+-----+------+\n\n+----+-----+------+\n|name|state|gender|\n+----+-----+------+\n|Anna|   NY|     F|\n+----+-----+------+\n\n+----+-----+------+\n|name|state|gender|\n+----+-----+------+\n|Anna|   NY|     F|\n+----+-----+------+\n\n+----+-----+------+\n|name|state|gender|\n+----+-----+------+\n|Anna|   NY|     F|\n+----+-----+------+\n\n+----+-----+------+\n|name|state|gender|\n+----+-----+------+\n|Anna|   NY|     F|\n+----+-----+------+\n\n+-----+-----+------+\n| name|state|gender|\n+-----+-----+------+\n|James| null|     M|\n|Julia| null|  null|\n+-----+-----+------+\n\n+-----+-----+------+\n| name|state|gender|\n+-----+-----+------+\n|Julia| null|  null|\n+-----+-----+------+\n\n+----+-----+------+\n|name|state|gender|\n+----+-----+------+\n|Anna|   NY|     F|\n+----+-----+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType, ArrayType\nfrom pyspark.sql.functions import col,array_contains\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\narrayStructureData = [\n        ((\"James\",\"\",\"Smith\"),[\"Java\",\"Scala\",\"C++\"],\"OH\",\"M\"),\n        ((\"Anna\",\"Rose\",\"\"),[\"Spark\",\"Java\",\"C++\"],\"NY\",\"F\"),\n        ((\"Julia\",\"\",\"Williams\"),[\"CSharp\",\"VB\"],\"OH\",\"F\"),\n        ((\"Maria\",\"Anne\",\"Jones\"),[\"CSharp\",\"VB\"],\"NY\",\"M\"),\n        ((\"Jen\",\"Mary\",\"Brown\"),[\"CSharp\",\"VB\"],\"NY\",\"M\"),\n        ((\"Mike\",\"Mary\",\"Williams\"),[\"Python\",\"VB\"],\"OH\",\"M\")\n        ]\n        \narrayStructureSchema = StructType([\n        StructField('name', StructType([\n             StructField('firstname', StringType(), True),\n             StructField('middlename', StringType(), True),\n             StructField('lastname', StringType(), True)\n             ])),\n         StructField('languages', ArrayType(StringType()), True),\n         StructField('state', StringType(), True),\n         StructField('gender', StringType(), True)\n         ])\n\n\ndf = spark.createDataFrame(data = arrayStructureData, schema = arrayStructureSchema)\ndf.printSchema()\ndf.show(truncate=False)\n\n#Equals\ndf.filter(df.state == \"OH\") \\\n    .show(truncate=False)\n\n#Not equals\ndf.filter(~(df.state == \"OH\")) \\\n    .show(truncate=False)\ndf.filter(df.state != \"OH\") \\\n    .show(truncate=False)    \n    \ndf.filter(col(\"state\") == \"OH\") \\\n    .show(truncate=False)    \n    \ndf.filter(\"gender  == 'M'\") \\\n    .show(truncate=False)    \n\ndf.filter(\"gender  <> 'M'\") \\\n    .show(truncate=False)    \n\n#IS IN\nli=[\"OH\",\"CA\",\"DE\"]\ndf.filter(df.state.isin(li)).show()\n#IS NOT IN\ndf.filter(~df.state.isin(li)).show()\n\ndf.filter( (df.state  == \"OH\") & (df.gender  == \"M\") ) \\\n    .show(truncate=False)        \n\ndf.filter(array_contains(df.languages,\"Java\")) \\\n    .show(truncate=False)        \n\ndf.filter(df.name.lastname == \"Williams\") \\\n    .show(truncate=False) \n\ndf.filter(df.state.startswith(\"N\")).show()\ndf.filter(df.state.endswith(\"H\")).show()\ndf.filter(df.state.like(\"N%\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"f951261a-f436-4cd6-87e5-f1ffcef41c64","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- name: struct (nullable = true)\n |    |-- firstname: string (nullable = true)\n |    |-- middlename: string (nullable = true)\n |    |-- lastname: string (nullable = true)\n |-- languages: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- state: string (nullable = true)\n |-- gender: string (nullable = true)\n\n+----------------------+------------------+-----+------+\n|name                  |languages         |state|gender|\n+----------------------+------------------+-----+------+\n|{James, , Smith}      |[Java, Scala, C++]|OH   |M     |\n|{Anna, Rose, }        |[Spark, Java, C++]|NY   |F     |\n|{Julia, , Williams}   |[CSharp, VB]      |OH   |F     |\n|{Maria, Anne, Jones}  |[CSharp, VB]      |NY   |M     |\n|{Jen, Mary, Brown}    |[CSharp, VB]      |NY   |M     |\n|{Mike, Mary, Williams}|[Python, VB]      |OH   |M     |\n+----------------------+------------------+-----+------+\n\n+----------------------+------------------+-----+------+\n|name                  |languages         |state|gender|\n+----------------------+------------------+-----+------+\n|{James, , Smith}      |[Java, Scala, C++]|OH   |M     |\n|{Julia, , Williams}   |[CSharp, VB]      |OH   |F     |\n|{Mike, Mary, Williams}|[Python, VB]      |OH   |M     |\n+----------------------+------------------+-----+------+\n\n+--------------------+------------------+-----+------+\n|name                |languages         |state|gender|\n+--------------------+------------------+-----+------+\n|{Anna, Rose, }      |[Spark, Java, C++]|NY   |F     |\n|{Maria, Anne, Jones}|[CSharp, VB]      |NY   |M     |\n|{Jen, Mary, Brown}  |[CSharp, VB]      |NY   |M     |\n+--------------------+------------------+-----+------+\n\n+--------------------+------------------+-----+------+\n|name                |languages         |state|gender|\n+--------------------+------------------+-----+------+\n|{Anna, Rose, }      |[Spark, Java, C++]|NY   |F     |\n|{Maria, Anne, Jones}|[CSharp, VB]      |NY   |M     |\n|{Jen, Mary, Brown}  |[CSharp, VB]      |NY   |M     |\n+--------------------+------------------+-----+------+\n\n+----------------------+------------------+-----+------+\n|name                  |languages         |state|gender|\n+----------------------+------------------+-----+------+\n|{James, , Smith}      |[Java, Scala, C++]|OH   |M     |\n|{Julia, , Williams}   |[CSharp, VB]      |OH   |F     |\n|{Mike, Mary, Williams}|[Python, VB]      |OH   |M     |\n+----------------------+------------------+-----+------+\n\n+----------------------+------------------+-----+------+\n|name                  |languages         |state|gender|\n+----------------------+------------------+-----+------+\n|{James, , Smith}      |[Java, Scala, C++]|OH   |M     |\n|{Maria, Anne, Jones}  |[CSharp, VB]      |NY   |M     |\n|{Jen, Mary, Brown}    |[CSharp, VB]      |NY   |M     |\n|{Mike, Mary, Williams}|[Python, VB]      |OH   |M     |\n+----------------------+------------------+-----+------+\n\n+-------------------+------------------+-----+------+\n|name               |languages         |state|gender|\n+-------------------+------------------+-----+------+\n|{Anna, Rose, }     |[Spark, Java, C++]|NY   |F     |\n|{Julia, , Williams}|[CSharp, VB]      |OH   |F     |\n+-------------------+------------------+-----+------+\n\n+--------------------+------------------+-----+------+\n|                name|         languages|state|gender|\n+--------------------+------------------+-----+------+\n|    {James, , Smith}|[Java, Scala, C++]|   OH|     M|\n| {Julia, , Williams}|      [CSharp, VB]|   OH|     F|\n|{Mike, Mary, Will...|      [Python, VB]|   OH|     M|\n+--------------------+------------------+-----+------+\n\n+--------------------+------------------+-----+------+\n|                name|         languages|state|gender|\n+--------------------+------------------+-----+------+\n|      {Anna, Rose, }|[Spark, Java, C++]|   NY|     F|\n|{Maria, Anne, Jones}|      [CSharp, VB]|   NY|     M|\n|  {Jen, Mary, Brown}|      [CSharp, VB]|   NY|     M|\n+--------------------+------------------+-----+------+\n\n+----------------------+------------------+-----+------+\n|name                  |languages         |state|gender|\n+----------------------+------------------+-----+------+\n|{James, , Smith}      |[Java, Scala, C++]|OH   |M     |\n|{Mike, Mary, Williams}|[Python, VB]      |OH   |M     |\n+----------------------+------------------+-----+------+\n\n+----------------+------------------+-----+------+\n|name            |languages         |state|gender|\n+----------------+------------------+-----+------+\n|{James, , Smith}|[Java, Scala, C++]|OH   |M     |\n|{Anna, Rose, }  |[Spark, Java, C++]|NY   |F     |\n+----------------+------------------+-----+------+\n\n+----------------------+------------+-----+------+\n|name                  |languages   |state|gender|\n+----------------------+------------+-----+------+\n|{Julia, , Williams}   |[CSharp, VB]|OH   |F     |\n|{Mike, Mary, Williams}|[Python, VB]|OH   |M     |\n+----------------------+------------+-----+------+\n\n+--------------------+------------------+-----+------+\n|                name|         languages|state|gender|\n+--------------------+------------------+-----+------+\n|      {Anna, Rose, }|[Spark, Java, C++]|   NY|     F|\n|{Maria, Anne, Jones}|      [CSharp, VB]|   NY|     M|\n|  {Jen, Mary, Brown}|      [CSharp, VB]|   NY|     M|\n+--------------------+------------------+-----+------+\n\n+--------------------+------------------+-----+------+\n|                name|         languages|state|gender|\n+--------------------+------------------+-----+------+\n|    {James, , Smith}|[Java, Scala, C++]|   OH|     M|\n| {Julia, , Williams}|      [CSharp, VB]|   OH|     F|\n|{Mike, Mary, Will...|      [Python, VB]|   OH|     M|\n+--------------------+------------------+-----+------+\n\n+--------------------+------------------+-----+------+\n|                name|         languages|state|gender|\n+--------------------+------------------+-----+------+\n|      {Anna, Rose, }|[Spark, Java, C++]|   NY|     F|\n|{Maria, Anne, Jones}|      [CSharp, VB]|   NY|     M|\n|  {Jen, Mary, Brown}|      [CSharp, VB]|   NY|     M|\n+--------------------+------------------+-----+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\ndata2 = [(1,\"James Smith\"), (2,\"Michael Rose\"),\n    (3,\"Robert Williams\"), (4,\"Rames Rose\"),(5,\"Rames rose\")\n  ]\ndf2 = spark.createDataFrame(data = data2, schema = [\"id\",\"name\"])\n\ndf2.filter(df2.name.like(\"%rose%\")).show()\ndf2.filter(df2.name.rlike(\"(?i)^*rose$\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"9f091a01-8a37-4314-a56f-40c6c9a7d1df","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+----------+\n| id|      name|\n+---+----------+\n|  5|Rames rose|\n+---+----------+\n\n+---+------------+\n| id|        name|\n+---+------------+\n|  2|Michael Rose|\n|  4|  Rames Rose|\n|  5|  Rames rose|\n+---+------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\nspark = SparkSession.builder \\\n    .master(\"local[1]\") \\\n    .appName(\"SparkByExamples.com\") \\\n    .getOrCreate()\n\ndata = [\n    (\"James\",None,\"M\"),\n    (\"Anna\",\"NY\",\"F\"),\n    (\"Julia\",None,None)\n  ]\n\ncolumns = [\"name\",\"state\",\"gender\"]\ndf = spark.createDataFrame(data,columns)\ndf.show()\n\ndf.filter(\"state is NULL\").show()\ndf.filter(df.state.isNull()).show()\ndf.filter(col(\"state\").isNull()).show() \n\ndf.na.drop(\"state\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"270c01b3-ca93-4926-8104-508771759799","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----+-----+------+\n| name|state|gender|\n+-----+-----+------+\n|James| null|     M|\n| Anna|   NY|     F|\n|Julia| null|  null|\n+-----+-----+------+\n\n+-----+-----+------+\n| name|state|gender|\n+-----+-----+------+\n|James| null|     M|\n|Julia| null|  null|\n+-----+-----+------+\n\n+-----+-----+------+\n| name|state|gender|\n+-----+-----+------+\n|James| null|     M|\n|Julia| null|  null|\n+-----+-----+------+\n\n+-----+-----+------+\n| name|state|gender|\n+-----+-----+------+\n|James| null|     M|\n|Julia| null|  null|\n+-----+-----+------+\n\n"]},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\nFile \u001B[0;32m<command-3712906355083946>:22\u001B[0m\n\u001B[1;32m     19\u001B[0m df\u001B[38;5;241m.\u001B[39mfilter(df\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39misNull())\u001B[38;5;241m.\u001B[39mshow()\n\u001B[1;32m     20\u001B[0m df\u001B[38;5;241m.\u001B[39mfilter(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstate\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39misNull())\u001B[38;5;241m.\u001B[39mshow() \n\u001B[0;32m---> 22\u001B[0m df\u001B[38;5;241m.\u001B[39mna\u001B[38;5;241m.\u001B[39mdrop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstate\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mshow()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:5268\u001B[0m, in \u001B[0;36mDataFrameNaFunctions.drop\u001B[0;34m(self, how, thresh, subset)\u001B[0m\n\u001B[1;32m   5262\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdrop\u001B[39m(\n\u001B[1;32m   5263\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   5264\u001B[0m     how: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124many\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   5265\u001B[0m     thresh: Optional[\u001B[38;5;28mint\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   5266\u001B[0m     subset: Optional[Union[\u001B[38;5;28mstr\u001B[39m, Tuple[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m], List[\u001B[38;5;28mstr\u001B[39m]]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   5267\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame:\n\u001B[0;32m-> 5268\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdropna\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhow\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhow\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mthresh\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mthresh\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msubset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msubset\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:43\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m     40\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;241m*\u001B[39margs: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m     41\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(_local, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlogging\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m _local\u001B[38;5;241m.\u001B[39mlogging:\n\u001B[1;32m     42\u001B[0m         \u001B[38;5;66;03m# no need to log since this should be internal call.\u001B[39;00m\n\u001B[0;32m---> 43\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     44\u001B[0m     _local\u001B[38;5;241m.\u001B[39mlogging \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m     45\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3968\u001B[0m, in \u001B[0;36mDataFrame.dropna\u001B[0;34m(self, how, thresh, subset)\u001B[0m\n\u001B[1;32m   3925\u001B[0m \u001B[38;5;124;03m\"\"\"Returns a new :class:`DataFrame` omitting rows with null values.\u001B[39;00m\n\u001B[1;32m   3926\u001B[0m \u001B[38;5;124;03m:func:`DataFrame.dropna` and :func:`DataFrameNaFunctions.drop` are aliases of each other.\u001B[39;00m\n\u001B[1;32m   3927\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3965\u001B[0m \u001B[38;5;124;03m+---+------+-----+\u001B[39;00m\n\u001B[1;32m   3966\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   3967\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m how \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m how \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124many\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mall\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[0;32m-> 3968\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhow (\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m how \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m) should be \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124many\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m or \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mall\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   3970\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m subset \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   3971\u001B[0m     subset \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\n\n\u001B[0;31mValueError\u001B[0m: how ('state') should be 'any' or 'all'","errorSummary":"<span class='ansi-red-fg'>ValueError</span>: how ('state') should be 'any' or 'all'","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n","\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\n","File \u001B[0;32m<command-3712906355083946>:22\u001B[0m\n","\u001B[1;32m     19\u001B[0m df\u001B[38;5;241m.\u001B[39mfilter(df\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39misNull())\u001B[38;5;241m.\u001B[39mshow()\n","\u001B[1;32m     20\u001B[0m df\u001B[38;5;241m.\u001B[39mfilter(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstate\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39misNull())\u001B[38;5;241m.\u001B[39mshow() \n","\u001B[0;32m---> 22\u001B[0m df\u001B[38;5;241m.\u001B[39mna\u001B[38;5;241m.\u001B[39mdrop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstate\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mshow()\n","\n","File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n","\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n","\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n","\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n","\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n","\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n","\u001B[1;32m     51\u001B[0m     )\n","\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n","\n","File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:5268\u001B[0m, in \u001B[0;36mDataFrameNaFunctions.drop\u001B[0;34m(self, how, thresh, subset)\u001B[0m\n","\u001B[1;32m   5262\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdrop\u001B[39m(\n","\u001B[1;32m   5263\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n","\u001B[1;32m   5264\u001B[0m     how: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124many\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n","\u001B[1;32m   5265\u001B[0m     thresh: Optional[\u001B[38;5;28mint\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n","\u001B[1;32m   5266\u001B[0m     subset: Optional[Union[\u001B[38;5;28mstr\u001B[39m, Tuple[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m], List[\u001B[38;5;28mstr\u001B[39m]]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n","\u001B[1;32m   5267\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame:\n","\u001B[0;32m-> 5268\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdropna\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhow\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhow\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mthresh\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mthresh\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msubset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msubset\u001B[49m\u001B[43m)\u001B[49m\n","\n","File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:43\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n","\u001B[1;32m     39\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n","\u001B[1;32m     40\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;241m*\u001B[39margs: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n","\u001B[1;32m     41\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(_local, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlogging\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m _local\u001B[38;5;241m.\u001B[39mlogging:\n","\u001B[1;32m     42\u001B[0m         \u001B[38;5;66;03m# no need to log since this should be internal call.\u001B[39;00m\n","\u001B[0;32m---> 43\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n","\u001B[1;32m     44\u001B[0m     _local\u001B[38;5;241m.\u001B[39mlogging \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n","\u001B[1;32m     45\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n","\n","File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3968\u001B[0m, in \u001B[0;36mDataFrame.dropna\u001B[0;34m(self, how, thresh, subset)\u001B[0m\n","\u001B[1;32m   3925\u001B[0m \u001B[38;5;124;03m\"\"\"Returns a new :class:`DataFrame` omitting rows with null values.\u001B[39;00m\n","\u001B[1;32m   3926\u001B[0m \u001B[38;5;124;03m:func:`DataFrame.dropna` and :func:`DataFrameNaFunctions.drop` are aliases of each other.\u001B[39;00m\n","\u001B[1;32m   3927\u001B[0m \n","\u001B[0;32m   (...)\u001B[0m\n","\u001B[1;32m   3965\u001B[0m \u001B[38;5;124;03m+---+------+-----+\u001B[39;00m\n","\u001B[1;32m   3966\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n","\u001B[1;32m   3967\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m how \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m how \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124many\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mall\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n","\u001B[0;32m-> 3968\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhow (\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m how \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m) should be \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124many\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m or \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mall\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n","\u001B[1;32m   3970\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m subset \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n","\u001B[1;32m   3971\u001B[0m     subset \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\n","\n","\u001B[0;31mValueError\u001B[0m: how ('state') should be 'any' or 'all'"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col,sum,avg,max\n\nspark = SparkSession.builder \\\n                    .appName('SparkByExamples.com') \\\n                    .getOrCreate()\n\nsimpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000),\n    (\"Michael\",\"Sales\",\"NV\",86000,56,20000),\n    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n    (\"Raman\",\"Finance\",\"DE\",99000,40,24000),\n    (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n    (\"Jeff\",\"Marketing\",\"NV\",80000,25,18000),\n    (\"Kumar\",\"Marketing\",\"NJ\",91000,50,21000)\n  ]\n\nschema = [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\ndf = spark.createDataFrame(data=simpleData, schema = schema)\ndf.printSchema()\ndf.show(truncate=False)\n\ndf.groupBy(\"state\").sum(\"salary\").show()\n\ndfGroup=df.groupBy(\"state\") \\\n          .agg(sum(\"salary\").alias(\"sum_salary\"))\n          \ndfGroup.show(truncate=False)\n\ndfFilter=dfGroup.filter(dfGroup.sum_salary > 100000)\ndfFilter.show()\n\nfrom pyspark.sql.functions import asc\ndfFilter.sort(\"sum_salary\").show()\n\nfrom pyspark.sql.functions import desc\ndfFilter.sort(desc(\"sum_salary\")).show()\n\ndf.groupBy(\"state\") \\\n  .agg(sum(\"salary\").alias(\"sum_salary\")) \\\n  .filter(col(\"sum_salary\") > 100000)  \\\n  .sort(desc(\"sum_salary\")) \\\n  .show()\n  \ndf.createOrReplaceTempView(\"EMP\")\nspark.sql(\"select state, sum(salary) as sum_salary from EMP \" +\n          \"group by state having sum_salary > 100000 \" + \n          \"order by sum_salary desc\").show()\n\ndf.groupBy(\"state\") \\\n  .sum(\"salary\") \\\n  .withColumnRenamed(\"sum(salary)\", \"sum_salary\") \\\n  .show()\n\ndf.groupBy(\"state\") \\\n  .sum(\"salary\") \\\n  .select(col(\"state\"),col(\"sum(salary)\").alias(\"sum_salary\")) \\\n  .show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"64101991-0f22-40ab-ad9d-e6487214bb08","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- employee_name: string (nullable = true)\n |-- department: string (nullable = true)\n |-- state: string (nullable = true)\n |-- salary: long (nullable = true)\n |-- age: long (nullable = true)\n |-- bonus: long (nullable = true)\n\n+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|James        |Sales     |NY   |90000 |34 |10000|\n|Michael      |Sales     |NV   |86000 |56 |20000|\n|Robert       |Sales     |CA   |81000 |30 |23000|\n|Maria        |Finance   |CA   |90000 |24 |23000|\n|Raman        |Finance   |DE   |99000 |40 |24000|\n|Scott        |Finance   |NY   |83000 |36 |19000|\n|Jen          |Finance   |NY   |79000 |53 |15000|\n|Jeff         |Marketing |NV   |80000 |25 |18000|\n|Kumar        |Marketing |NJ   |91000 |50 |21000|\n+-------------+----------+-----+------+---+-----+\n\n+-----+-----------+\n|state|sum(salary)|\n+-----+-----------+\n|   NY|     252000|\n|   NV|     166000|\n|   CA|     171000|\n|   DE|      99000|\n|   NJ|      91000|\n+-----+-----------+\n\n+-----+----------+\n|state|sum_salary|\n+-----+----------+\n|NY   |252000    |\n|NV   |166000    |\n|CA   |171000    |\n|DE   |99000     |\n|NJ   |91000     |\n+-----+----------+\n\n+-----+----------+\n|state|sum_salary|\n+-----+----------+\n|   NY|    252000|\n|   NV|    166000|\n|   CA|    171000|\n+-----+----------+\n\n+-----+----------+\n|state|sum_salary|\n+-----+----------+\n|   NV|    166000|\n|   CA|    171000|\n|   NY|    252000|\n+-----+----------+\n\n+-----+----------+\n|state|sum_salary|\n+-----+----------+\n|   NY|    252000|\n|   CA|    171000|\n|   NV|    166000|\n+-----+----------+\n\n+-----+----------+\n|state|sum_salary|\n+-----+----------+\n|   NY|    252000|\n|   CA|    171000|\n|   NV|    166000|\n+-----+----------+\n\n+-----+----------+\n|state|sum_salary|\n+-----+----------+\n|   NY|    252000|\n|   CA|    171000|\n|   NV|    166000|\n+-----+----------+\n\n+-----+----------+\n|state|sum_salary|\n+-----+----------+\n|   NY|    252000|\n|   NV|    166000|\n|   CA|    171000|\n|   DE|     99000|\n|   NJ|     91000|\n+-----+----------+\n\n+-----+----------+\n|state|sum_salary|\n+-----+----------+\n|   NY|    252000|\n|   NV|    166000|\n|   CA|    171000|\n|   DE|     99000|\n|   NJ|     91000|\n+-----+----------+\n\n"]}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Untitled Notebook 2023-06-12 22:41:30","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
