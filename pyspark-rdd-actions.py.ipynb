{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\ndata=[(\"Z\", 1),(\"A\", 20),(\"B\", 30),(\"C\", 40),(\"B\", 30),(\"B\", 60)]\ninputRDD = spark.sparkContext.parallelize(data)\n  \nlistRdd = spark.sparkContext.parallelize([1,2,3,4,5,3,2])\n\n#aggregate\nseqOp = (lambda x, y: x + y)\ncombOp = (lambda x, y: x + y)\nagg=listRdd.aggregate(0, seqOp, combOp)\nprint(agg) # output 20\n\n#aggregate 2\nseqOp2 = (lambda x, y: (x[0] + y, x[1] + 1))\ncombOp2 = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\nagg2=listRdd.aggregate((0, 0), seqOp2, combOp2)\nprint(agg2) # output (20,7)\n\nagg2=listRdd.treeAggregate(0,seqOp, combOp)\nprint(agg2) # output 20\n\n#fold\nfrom operator import add\nfoldRes=listRdd.fold(0, add)\nprint(foldRes) # output 20\n\n#reduce\nredRes=listRdd.reduce(add)\nprint(redRes) # output 20\n\n#treeReduce. This is similar to reduce\nadd = lambda x, y: x + y\nredRes=listRdd.treeReduce(add)\nprint(redRes) # output 20\n\n#Collect\ndata = listRdd.collect()\nprint(data)\n\n#count, countApprox, countApproxDistinct\nprint(\"Count : \"+str(listRdd.count()))\n#Output: Count : 20\nprint(\"countApprox : \"+str(listRdd.countApprox(1200)))\n#Output: countApprox : (final: [7.000, 7.000])\nprint(\"countApproxDistinct : \"+str(listRdd.countApproxDistinct()))\n#Output: countApproxDistinct : 5\nprint(\"countApproxDistinct : \"+str(inputRDD.countApproxDistinct()))\n#Output: countApproxDistinct : 5\n\n#countByValue, countByValueApprox\nprint(\"countByValue :  \"+str(listRdd.countByValue()))\n\n\n#first\nprint(\"first :  \"+str(listRdd.first()))\n#Output: first :  1\nprint(\"first :  \"+str(inputRDD.first()))\n#Output: first :  (Z,1)\n\n#top\nprint(\"top : \"+str(listRdd.top(2)))\n#Output: take : 5,4\nprint(\"top : \"+str(inputRDD.top(2)))\n#Output: take : (Z,1),(C,40)\n\n#min\nprint(\"min :  \"+str(listRdd.min()))\n#Output: min :  1\nprint(\"min :  \"+str(inputRDD.min()))\n#Output: min :  (A,20)  \n\n#max\nprint(\"max :  \"+str(listRdd.max()))\n#Output: max :  5\nprint(\"max :  \"+str(inputRDD.max()))\n#Output: max :  (Z,1)\n\n#take, takeOrdered, takeSample\nprint(\"take : \"+str(listRdd.take(2)))\n#Output: take : 1,2\nprint(\"takeOrdered : \"+ str(listRdd.takeOrdered(2)))\n#Output: takeOrdered : 1,2\nprint(\"take : \"+str(listRdd.takeSample()))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"f64ff43e-8bcf-4669-9dd5-cf98b43f990c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["20\n(20, 7)\n20\n20\n20\n20\n[1, 2, 3, 4, 5, 3, 2]\nCount : 7\ncountApprox : 7\ncountApproxDistinct : 5\ncountApproxDistinct : 5\ncountByValue :  defaultdict(<class 'int'>, {1: 1, 2: 2, 3: 2, 4: 1, 5: 1})\nfirst :  1\nfirst :  ('Z', 1)\ntop : [5, 4]\ntop : [('Z', 1), ('C', 40)]\nmin :  1\nmin :  ('A', 20)\nmax :  5\nmax :  ('Z', 1)\ntake : [1, 2]\ntakeOrdered : [1, 2]\n"]},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-2820217326053897>:84\u001B[0m\n\u001B[1;32m     82\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtakeOrdered : \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(listRdd\u001B[38;5;241m.\u001B[39mtakeOrdered(\u001B[38;5;241m2\u001B[39m)))\n\u001B[1;32m     83\u001B[0m \u001B[38;5;66;03m#Output: takeOrdered : 1,2\u001B[39;00m\n\u001B[0;32m---> 84\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtake : \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;28mstr\u001B[39m(listRdd\u001B[38;5;241m.\u001B[39mtakeSample()))\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\n\u001B[0;31mTypeError\u001B[0m: takeSample() missing 2 required positional arguments: 'withReplacement' and 'num'","errorSummary":"<span class='ansi-red-fg'>TypeError</span>: takeSample() missing 2 required positional arguments: 'withReplacement' and 'num'","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n","\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n","File \u001B[0;32m<command-2820217326053897>:84\u001B[0m\n","\u001B[1;32m     82\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtakeOrdered : \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(listRdd\u001B[38;5;241m.\u001B[39mtakeOrdered(\u001B[38;5;241m2\u001B[39m)))\n","\u001B[1;32m     83\u001B[0m \u001B[38;5;66;03m#Output: takeOrdered : 1,2\u001B[39;00m\n","\u001B[0;32m---> 84\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtake : \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;28mstr\u001B[39m(listRdd\u001B[38;5;241m.\u001B[39mtakeSample()))\n","\n","File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n","\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n","\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n","\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n","\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n","\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n","\u001B[1;32m     51\u001B[0m     )\n","\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n","\n","\u001B[0;31mTypeError\u001B[0m: takeSample() missing 2 required positional arguments: 'withReplacement' and 'num'"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark-rdd-actions.py","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
