{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\n\n# Create SparkSession\nspark = SparkSession.builder \\\n          .appName('SparkByExamples.com') \\\n          .getOrCreate()\n\ninputData = [(\"2019-07-01 12:01:19\",\n            \"07-01-2019 12:01:19\", \n            \"07-01-2019\")]\ncolumns=[\"timestamp_1\",\"timestamp_2\",\"timestamp_3\"]\ndf=spark.createDataFrame(\n        data = inputData,\n        schema = columns)\ndf.printSchema()\ndf.show(truncate=False)\n\nfrom pyspark.sql.functions import *\ndf2 = df.select( \n      unix_timestamp(col(\"timestamp_1\")).alias(\"timestamp_1\"), \n      unix_timestamp(col(\"timestamp_2\"),\"MM-dd-yyyy HH:mm:ss\").alias(\"timestamp_2\"), \n      unix_timestamp(col(\"timestamp_3\"),\"MM-dd-yyyy\").alias(\"timestamp_3\"), \n      unix_timestamp().alias(\"timestamp_4\") \n   )\ndf2.printSchema()\ndf2.show(truncate=False)\n\ndf3=df2.select(\n    from_unixtime(col(\"timestamp_1\")).alias(\"timestamp_1\"),\n    from_unixtime(col(\"timestamp_2\"),\"MM-dd-yyyy HH:mm:ss\").alias(\"timestamp_2\"),\n    from_unixtime(col(\"timestamp_3\"),\"MM-dd-yyyy\").alias(\"timestamp_3\"),\n    from_unixtime(col(\"timestamp_4\")).alias(\"timestamp_4\")\n  )\ndf3.printSchema()\ndf3.show(truncate=False)\n\n#SQL"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"8b9045da-3d43-4945-a41e-dbe3d38c1e58","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- timestamp_1: string (nullable = true)\n |-- timestamp_2: string (nullable = true)\n |-- timestamp_3: string (nullable = true)\n\n+-------------------+-------------------+-----------+\n|timestamp_1        |timestamp_2        |timestamp_3|\n+-------------------+-------------------+-----------+\n|2019-07-01 12:01:19|07-01-2019 12:01:19|07-01-2019 |\n+-------------------+-------------------+-----------+\n\nroot\n |-- timestamp_1: long (nullable = true)\n |-- timestamp_2: long (nullable = true)\n |-- timestamp_3: long (nullable = true)\n |-- timestamp_4: long (nullable = true)\n\n+-----------+-----------+-----------+-----------+\n|timestamp_1|timestamp_2|timestamp_3|timestamp_4|\n+-----------+-----------+-----------+-----------+\n|1561982479 |1561982479 |1561939200 |1687321701 |\n+-----------+-----------+-----------+-----------+\n\nroot\n |-- timestamp_1: string (nullable = true)\n |-- timestamp_2: string (nullable = true)\n |-- timestamp_3: string (nullable = true)\n |-- timestamp_4: string (nullable = true)\n\n+-------------------+-------------------+-----------+-------------------+\n|timestamp_1        |timestamp_2        |timestamp_3|timestamp_4        |\n+-------------------+-------------------+-----------+-------------------+\n|2019-07-01 12:01:19|07-01-2019 12:01:19|07-01-2019 |2023-06-21 04:28:21|\n+-------------------+-------------------+-----------+-------------------+\n\n"]}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark-unix-time.py","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
